{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train image dataset has shape: (60000, 28, 28)\n",
      "The test image dataset has shape: (10000, 28, 28)\n",
      "The train label dataset has shape: 60000\n",
      "The test label dataset has shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print('The train image dataset has shape:', train_images.shape)\n",
    "print('The test image dataset has shape:',test_images.shape)\n",
    "print('The train label dataset has shape:', len(train_labels))\n",
    "print('The test label dataset has shape:',test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "change the datashet to 28x28=784 vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 784)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = 20000 # size of sample from the training data\n",
    "nsamples, nx, ny = train_images.shape\n",
    "train_images_vector = train_images.reshape((nsamples,nx*ny))\n",
    "train_images_vector.shape\n",
    "train_images_vector = train_images_vector[0:size]\n",
    "train_images_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize clusters vector to 0 and centroids vector to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_images_vector\n",
    "K = 10\n",
    "clusters = np.full(size, 0)   \n",
    "centroids = np.zeros((10, 784))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "put initial values in centroids and clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6314 16903 16632 10594 15650 15313 19703 13612 10565 16142]\n"
     ]
    }
   ],
   "source": [
    "indexes = np.random.choice(size, K, replace=False)\n",
    "print(indexes)\n",
    "for i in range(0, len(indexes)):\n",
    "    centroids[i] = X[indexes[i]]\n",
    "    clusters[indexes[i]] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kmeans with Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_cluster = 0\n",
    "for k in range(0, 10):\n",
    "    for i in range(X.shape[0]):\n",
    "        min_distance = np.sum(np.square(X[0] - centroids[0]))\n",
    "        temp_cluster = 0                    \n",
    "        for j in range(K):    \n",
    "            if min_distance > np.sum(np.square(X[i] - centroids[j])):\n",
    "                min_distance = np.sum(np.square(X[i] - centroids[j]))\n",
    "                temp_cluster = j\n",
    "                              \n",
    "        clusters[i] = temp_cluster\n",
    "\n",
    "    # find the middle of the cluster\n",
    "    for i in range(K):\n",
    "        data_index = np.where(clusters == i)\n",
    "        if(len(data_index[0]) != 0):\n",
    "#             print(\"class: \",  i)\n",
    "#             print(\"indexes: \", data_index[0])\n",
    "            sum_data = 0\n",
    "            for pos in data_index[0]:\n",
    "                sum_data = sum_data + X[pos]        \n",
    "            centroids[i] = sum_data / len(data_index[0])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Purity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1335"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = train_labels[0:size]\n",
    "correct_predictions = np.where(clusters == train_labels)\n",
    "purity = len(correct_predictions[0]) / size\n",
    "purity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate F- measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7800361336946703"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_f1_scores = []\n",
    "for cluster_category in range(0, 10):\n",
    "    current_cluster_index = np.where(clusters == cluster_category)\n",
    "\n",
    "    true_current_labels = []\n",
    "    for x in current_cluster_index[0]:\n",
    "        true_current_labels.append(train_labels[x])\n",
    "\n",
    "    # find pleiopsifiotita pou einai to positive\n",
    "    freq = {}    \n",
    "    for item in true_current_labels:  \n",
    "        if (item in freq):\n",
    "            freq[item] += 1\n",
    "        else:\n",
    "            freq[item] = 1\n",
    "\n",
    "    positive = max(freq, key=freq.get)\n",
    "\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for i in true_current_labels:\n",
    "        if(i == cluster_category):\n",
    "            if(i == positive):\n",
    "                tp =tp + 1\n",
    "            else:\n",
    "                tn = tn + 1\n",
    "        else:\n",
    "            if(i == positive):\n",
    "                fp =fp + 1\n",
    "            else:\n",
    "                fn = fn + 1\n",
    "    \n",
    "    if (tp + fp) != 0:\n",
    "        precision = tp / (tp + fp)\n",
    "    if (tp + fn) != 0:    \n",
    "        recall = tp / (tp + fn)\n",
    "    \n",
    "    if(precision == recall == 0):\n",
    "        f1 = 0\n",
    "    else:   \n",
    "        f1 = 2* (precision*recall) / (precision+recall)\n",
    "\n",
    "    all_f1_scores.append(f1)\n",
    "total_f1 = sum(all_f1_scores)\n",
    "total_f1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
